{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5868d66",
   "metadata": {},
   "source": [
    "<h1>Classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395d2066",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Logistic Regression</h2><h3>Parameters study</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db5e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, ndarray\n",
    "from matplotlib.pyplot import figure, savefig, show\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from dslabs_functions import (\n",
    "    CLASS_EVAL_METRICS,\n",
    "    DELTA_IMPROVE,\n",
    "    read_train_test_from_files,\n",
    ")\n",
    "from dslabs_functions import plot_evaluation_results, plot_multiline_chart\n",
    "\n",
    "\n",
    "def logistic_regression_study(\n",
    "    trnX: ndarray,\n",
    "    trnY: array,\n",
    "    tstX: ndarray,\n",
    "    tstY: array,\n",
    "    nr_max_iterations: int = 2500,\n",
    "    lag: int = 500,\n",
    "    metric: str = \"accuracy\",\n",
    ") -> tuple[LogisticRegression | None, dict]:\n",
    "    nr_iterations: list[int] = [lag] + [\n",
    "        i for i in range(2 * lag, nr_max_iterations + 1, lag)\n",
    "    ]\n",
    "\n",
    "    penalty_types: list[str] = [\"l1\", \"l2\"]  # only available if optimizer='liblinear'\n",
    "\n",
    "    best_model = None\n",
    "    best_params: dict = {\"name\": \"LR\", \"metric\": metric, \"params\": ()}\n",
    "    best_performance: float = 0.0\n",
    "\n",
    "    values: dict = {}\n",
    "    for type in penalty_types:\n",
    "        warm_start = False\n",
    "        y_tst_values: list[float] = []\n",
    "        for j in range(len(nr_iterations)):\n",
    "            clf = LogisticRegression(\n",
    "                penalty=type,\n",
    "                max_iter=lag,\n",
    "                warm_start=warm_start,\n",
    "                solver=\"liblinear\",\n",
    "                verbose=False,\n",
    "            )\n",
    "            clf.fit(trnX, trnY)\n",
    "            prdY: array = clf.predict(tstX)\n",
    "            eval: float = CLASS_EVAL_METRICS[metric](tstY, prdY)\n",
    "            y_tst_values.append(eval)\n",
    "            warm_start = True\n",
    "            if eval - best_performance > DELTA_IMPROVE:\n",
    "                best_performance = eval\n",
    "                best_params[\"params\"] = (type, nr_iterations[j])\n",
    "                best_model: LogisticRegression = clf\n",
    "            # print(f'MLP lr_type={type} lr={lr} n={nr_iterations[j]}')\n",
    "        values[type] = y_tst_values\n",
    "    plot_multiline_chart(\n",
    "        nr_iterations,\n",
    "        values,\n",
    "        title=f\"LR models ({metric})\",\n",
    "        xlabel=\"nr iterations\",\n",
    "        ylabel=metric,\n",
    "        percentage=True,\n",
    "    )\n",
    "    print(\n",
    "        f'LR best for {best_params[\"params\"][1]} iterations (penalty={best_params[\"params\"][0]})'\n",
    "    )\n",
    "\n",
    "    return best_model, best_params\n",
    "\n",
    "\n",
    "file_tag = \"stroke\"\n",
    "train_filename = \"data/stroke_train_smote.csv\"\n",
    "test_filename = \"data/stroke_test.csv\"\n",
    "target = \"stroke\"\n",
    "eval_metric = \"accuracy\"\n",
    "\n",
    "trnX, tstX, trnY, tstY, labels, vars = read_train_test_from_files(\n",
    "    train_filename, test_filename, target\n",
    ")\n",
    "print(f\"Train#={len(trnX)} Test#={len(tstX)}\")\n",
    "print(f\"Labels={labels}\")\n",
    "\n",
    "figure()\n",
    "best_model, params = logistic_regression_study(\n",
    "    trnX,\n",
    "    trnY,\n",
    "    tstX,\n",
    "    tstY,\n",
    "    nr_max_iterations=5000,\n",
    "    lag=500,\n",
    "    metric=eval_metric,\n",
    ")\n",
    "savefig(f\"images/{file_tag}_lr_{eval_metric}_study.png\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64589719",
   "metadata": {},
   "source": [
    "<h3>Best model performance</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8515fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_trn: array = best_model.predict(trnX)\n",
    "prd_tst: array = best_model.predict(tstX)\n",
    "figure()\n",
    "plot_evaluation_results(params, trnY, prd_trn, tstY, prd_tst, labels)\n",
    "savefig(f'images/{file_tag}_lr_{params[\"name\"]}_best_{params[\"metric\"]}_eval.png')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89e593",
   "metadata": {},
   "source": [
    "<h3>Overfitting study</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99780a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type: str = params[\"params\"][0]\n",
    "nr_iterations: list[int] = [i for i in range(100, 1001, 100)]\n",
    "\n",
    "y_tst_values: list[float] = []\n",
    "y_trn_values: list[float] = []\n",
    "acc_metric = \"accuracy\"\n",
    "\n",
    "warm_start = False\n",
    "for n in nr_iterations:\n",
    "    clf = LogisticRegression(\n",
    "        warm_start=warm_start,\n",
    "        penalty=type,\n",
    "        max_iter=n,\n",
    "        solver=\"liblinear\",\n",
    "        verbose=False,\n",
    "    )\n",
    "    clf.fit(trnX, trnY)\n",
    "    prd_tst_Y: array = clf.predict(tstX)\n",
    "    prd_trn_Y: array = clf.predict(trnX)\n",
    "    y_tst_values.append(CLASS_EVAL_METRICS[acc_metric](tstY, prd_tst_Y))\n",
    "    y_trn_values.append(CLASS_EVAL_METRICS[acc_metric](trnY, prd_trn_Y))\n",
    "    warm_start = True\n",
    "\n",
    "figure()\n",
    "plot_multiline_chart(\n",
    "    nr_iterations,\n",
    "    {\"Train\": y_trn_values, \"Test\": y_tst_values},\n",
    "    title=f\"LR overfitting study for penalty={type}\",\n",
    "    xlabel=\"nr_iterations\",\n",
    "    ylabel=str(eval_metric),\n",
    "    percentage=True,\n",
    ")\n",
    "savefig(f\"images/{file_tag}_lr_{eval_metric}_overfitting.png\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
