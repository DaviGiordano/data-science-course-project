{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1f047e",
   "metadata": {},
   "source": [
    "<h1>Classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b1adf",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Training Strategies</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb03eec",
   "metadata": {},
   "source": [
    "<h3>train_test_split function</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f6e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, ndarray\n",
    "from pandas import read_csv, DataFrame\n",
    "\n",
    "file_tag = \"stroke\"\n",
    "index_col = \"id\"\n",
    "target = \"stroke\"\n",
    "data: DataFrame = read_csv(\"data/stroke_mvi_encoded.csv\", index_col=index_col)\n",
    "labels: list = list(data[target].unique())\n",
    "labels.sort()\n",
    "print(f\"Labels={labels}\")\n",
    "\n",
    "positive: int = 1\n",
    "negative: int = 0\n",
    "values: dict[str, list[int]] = {\n",
    "    \"Original\": [\n",
    "        len(data[data[target] == negative]),\n",
    "        len(data[data[target] == positive]),\n",
    "    ]\n",
    "}\n",
    "\n",
    "y: array = data.pop(target).to_list()\n",
    "X: ndarray = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66610a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import concat\n",
    "from matplotlib.pyplot import figure, show\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dslabs_functions import plot_multibar_chart\n",
    "\n",
    "\n",
    "trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "\n",
    "train: DataFrame = concat(\n",
    "    [DataFrame(trnX, columns=data.columns), DataFrame(trnY, columns=[target])], axis=1\n",
    ")\n",
    "train.to_csv(f\"data/{file_tag}_train.csv\", index=False)\n",
    "\n",
    "test: DataFrame = concat(\n",
    "    [DataFrame(tstX, columns=data.columns), DataFrame(tstY, columns=[target])], axis=1\n",
    ")\n",
    "test.to_csv(f\"data/{file_tag}_test.csv\", index=False)\n",
    "\n",
    "values[\"Train\"] = [\n",
    "    len(train[train[target] == negative]),\n",
    "    len(train[train[target] == positive]),\n",
    "]\n",
    "values[\"Test\"] = [\n",
    "    len(test[test[target] == negative]),\n",
    "    len(test[test[target] == positive]),\n",
    "]\n",
    "\n",
    "figure(figsize=(6, 4))\n",
    "plot_multibar_chart(labels, values, title=\"Data distribution per dataset\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f105a0",
   "metadata": {},
   "source": [
    "<h3>Reading Train and Test datasets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "\n",
    "def read_train_test_from_files(\n",
    "    train_fn: str, test_fn: str, target: str = \"class\"\n",
    ") -> tuple[ndarray, ndarray, array, array, list, list]:\n",
    "    train: DataFrame = read_csv(train_fn, index_col=None)\n",
    "    labels: list = list(train[target].unique())\n",
    "    labels.sort()\n",
    "    trnY: array = train.pop(target).to_list()\n",
    "    trnX: ndarray = train.values\n",
    "\n",
    "    test: DataFrame = read_csv(test_fn, index_col=None)\n",
    "    tstY: array = test.pop(target).to_list()\n",
    "    tstX: ndarray = test.values\n",
    "    return trnX, tstX, trnY, tstY, labels, train.columns.to_list()\n",
    "\n",
    "\n",
    "file_tag = \"stroke\"\n",
    "train_filename = \"data/stroke_train_smote.csv\"\n",
    "test_filename = \"data/stroke_test.csv\"\n",
    "target = \"stroke\"\n",
    "eval_metric = \"accuracy\"\n",
    "\n",
    "trnX: ndarray\n",
    "tstX: ndarray\n",
    "trnY: array\n",
    "tstY: array\n",
    "labels: list\n",
    "vars: list\n",
    "trnX, tstX, trnY, tstY, labels, vars = read_train_test_from_files(\n",
    "    train_filename, test_filename, target\n",
    ")\n",
    "print(f\"Train#={len(trnX)} Test#={len(tstX)}\")\n",
    "print(f\"Labels={labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20a9401",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Estimators and Models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(trnX, trnY)\n",
    "pred_trnY: array = clf.predict(trnX)\n",
    "print(f\"Score over Train: {clf.score(trnX, trnY):.3f}\")\n",
    "print(f\"Score over Test: {clf.score(tstX, tstY):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395229a1",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Evaluation</h2><h3>Accuracy, Recall and Precision</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "pred_tstY: array = clf.predict(tstX)\n",
    "\n",
    "acc: float = accuracy_score(tstY, pred_tstY)\n",
    "recall: float = recall_score(tstY, pred_tstY)\n",
    "prec: float = precision_score(tstY, pred_tstY)\n",
    "print(f\"accuracy={acc:.3f} recall={recall:.3f} precision={prec:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc863f49",
   "metadata": {},
   "source": [
    "<h3>Confusion Matrix</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97601e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import unique\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels: list = list(unique(tstY))\n",
    "labels.sort()\n",
    "\n",
    "prdY: array = clf.predict(tstX)\n",
    "cnf_mtx_tst: ndarray = confusion_matrix(tstY, prdY, labels=labels)\n",
    "print(cnf_mtx_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f34793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from numpy import ndarray, set_printoptions, arange\n",
    "from matplotlib.pyplot import gca, cm\n",
    "from matplotlib.axes import Axes\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cnf_matrix: ndarray, classes_names: ndarray, ax: Axes = None) -> Axes:  # type: ignore\n",
    "    if ax is None:\n",
    "        ax = gca()\n",
    "    title = \"Confusion matrix\"\n",
    "    set_printoptions(precision=2)\n",
    "    tick_marks: ndarray = arange(0, len(classes_names), 1)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"True label\")\n",
    "    ax.set_xlabel(\"Predicted label\")\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_xticklabels(classes_names)\n",
    "    ax.set_yticklabels(classes_names)\n",
    "    ax.imshow(cnf_matrix, interpolation=\"nearest\", cmap=cm.Blues)\n",
    "\n",
    "    for i, j in product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "        ax.text(\n",
    "            j, i, format(cnf_matrix[i, j], \"d\"), color=\"y\", horizontalalignment=\"center\"\n",
    "        )\n",
    "    return ax\n",
    "\n",
    "\n",
    "figure()\n",
    "plot_confusion_matrix(cnf_mtx_tst, labels)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4327b799",
   "metadata": {},
   "source": [
    "<h3>ROC Charts</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a4a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "from config import ACTIVE_COLORS\n",
    "\n",
    "\n",
    "def plot_roc_chart(tstY: ndarray, predictions: dict, ax: Axes = None, target: str = \"class\") -> Axes:  # type: ignore\n",
    "    if ax is None:\n",
    "        ax = gca()\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.set_xlabel(\"FP rate\")\n",
    "    ax.set_ylabel(\"TP rate\")\n",
    "    ax.set_title(\"ROC chart for %s\" % target)\n",
    "\n",
    "    ax.plot(\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "        color=\"navy\",\n",
    "        label=\"random\",\n",
    "        linewidth=1,\n",
    "        linestyle=\"--\",\n",
    "        marker=\"\",\n",
    "    )\n",
    "    models = list(predictions.keys())\n",
    "    for i in range(len(models)):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_true=tstY,\n",
    "            y_pred=predictions[models[i]],\n",
    "            name=models[i],\n",
    "            ax=ax,\n",
    "            color=ACTIVE_COLORS[i],\n",
    "            linewidth=1,\n",
    "        )\n",
    "    ax.legend(loc=\"lower right\", fontsize=\"xx-small\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "figure()\n",
    "plot_roc_chart(tstY, {\"GaussianNB\": prdY}, target=target)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95c1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.pyplot import subplots, savefig, figure\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from dslabs_functions import plot_multibar_chart, HEIGHT\n",
    "\n",
    "CLASS_EVAL_METRICS: dict[str, Callable] = {\n",
    "    \"accuracy\": accuracy_score,\n",
    "    \"recall\": recall_score,\n",
    "    \"precision\": precision_score,\n",
    "    \"auc\": roc_auc_score,\n",
    "    \"f1\": f1_score,\n",
    "}\n",
    "\n",
    "\n",
    "def plot_evaluation_results(\n",
    "    model, trn_y, prd_trn, tst_y, prd_tst, labels: ndarray\n",
    ") -> ndarray:\n",
    "    evaluation: dict = {}\n",
    "    for key in CLASS_EVAL_METRICS:\n",
    "        evaluation[key] = [\n",
    "            CLASS_EVAL_METRICS[key](trn_y, prd_trn),\n",
    "            CLASS_EVAL_METRICS[key](tst_y, prd_tst),\n",
    "        ]\n",
    "\n",
    "    params_st: str = \"\" if () == model[\"params\"] else str(model[\"params\"])\n",
    "    fig: Figure\n",
    "    axs: ndarray\n",
    "    fig, axs = subplots(1, 2, figsize=(2 * HEIGHT, HEIGHT))\n",
    "    fig.suptitle(f'Best {model[\"metric\"]} for {model[\"name\"]} {params_st}')\n",
    "    plot_multibar_chart([\"Train\", \"Test\"], evaluation, ax=axs[0], percentage=True)\n",
    "\n",
    "    cnf_mtx_tst: ndarray = confusion_matrix(tst_y, prd_tst, labels=labels)\n",
    "    plot_confusion_matrix(cnf_mtx_tst, labels, ax=axs[1])\n",
    "    return axs\n",
    "\n",
    "\n",
    "model_description: dict = {\"name\": \"GaussianNB\", \"metric\": eval_metric, \"params\": ()}\n",
    "\n",
    "prd_trn: array = clf.predict(trnX)\n",
    "prd_tst: array = clf.predict(tstX)\n",
    "figure()\n",
    "plot_evaluation_results(model_description, trnY, prd_trn, tstY, prd_tst, labels)\n",
    "savefig(\n",
    "    f'images/{file_tag}_{model_description[\"name\"]}_best_{model_description[\"metric\"]}_eval.png'\n",
    ")\n",
    "\n",
    "show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
